# -*- coding: utf-8 -*-
"""tp_deep_learning_cnn_part_3_for_students.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L0ENJXJBEQIKA9E7Fy50ArZhUfFK4bhQ

# TP CNN, part 3 : super-resolution

Author : Alasdair Newson
alasdair.newson@telecom-paris.fr
 
## Objective:

We want to implement a Convolutional Neural Network (CNN) to do image super-resolution.

## Image super-resolution:

The super-resolution problem can be summarised as follows. We have an image as an input, which is defined over a grid $\{0,1,\dots, m-1\} \times \{0,1,\dots, n-1\}$. We define a factor $\delta$, by which we upsample the image. The output of the super-resolution is an image defined on the grid $\{0,\frac{1}{\delta},\dots, m-1\} \times \{0,\frac{1}{\delta},\dots, n-1\}$.

## Model

In this part of the TP, you have complete freedom to create any model you want, as long as the input is an image, and the output is also an image of size $\delta m \times \delta n$. You will have to choose the architecture and loss which seems reasonable to you.

To help you, here is a function to upsample images in neural networks :

- ```from tensorflow.keras.layers import UpSampling2D```

Of course, you can use any upsampling layer you wish. 

## Dataset

We will be using the mnist dataset for this part. This is to ensure that you can obtain good results. The input data should be the subsampled version of the mnist images, subsampled by taking one out of every $\delta$ pixels. The output data should be the normal-resolution mnist images.

__IMPORTANT NOTES:__
- Think carefully about what the training data and labels are in this case, and create them accordingly
- We will use ```n_max=5000``` to limit the number of datapoints (as in part 1) to go faster
- We set $\delta$ to 2 in this TP, because it is not too difficult to create a network that works with this factor. If you change it, it might be more difficult to create a satisfactory network.

# Your task:
You have to load the mnist data (see the first part of the TP), create the model, train it, and evaluate and display the results.

We have created a function ```super_res_interpolate```, which carries out super-resolution using basic interpolation (bilinear or bicubic), with which you can compare your results visually and numerically.
"""

# Commented out IPython magic to ensure Python compatibility.

# # Load packages

# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

import tensorflow as tf
from tensorflow.keras.utils import to_categorical

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input
from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras import optimizers
from scipy import interpolate
print(tf.keras.__version__)

"""This next cell is the only code you are given to carry out the TP. This function carries out a bilinear upsampling, with which you can compare your super-resolution. This function is __not__ supposed to be used by you in your network."""

# choice of the interpolation method
interp_method = 'linear'
# upsampling factor
delta = 2
# the maximum number of data to take from mnist (to go a bit faster)
n_max = 5000

# upsample by a factor of delta
# by definition, the new grid has a step size of 1/delta
def super_res_interpolate(imgs_in,delta,interp_method = 'linear'):
	imgs_out = tf.image.resize( tf.constant(imgs_in),\
		[delta*imgs_in.shape[1],delta*imgs_in.shape[2]], method='bilinear').numpy()

	return(imgs_out)

"""## Create your super-resolution network

The rest is up to you ! Import the data, format it (you can use the first part of the TP as help), create your network, train it, and compare the results with ```super_res_interpolate```.

Your network should be able to achieve about $80\%$ accuracy.

__Note__ you can obviously create as many cells as you like in your work.

### Récupération des données

On importe MNIST et on le formate comme dans le TP 1
"""

from keras.datasets import mnist
(X_train, Y_train_scalar), (X_test, Y_test_scalar) = mnist.load_data()

n_max = 5000
X_train = X_train[0:n_max, :, :]
X_test = X_test[0:n_max, :, :]
Y_train_scalar = Y_train_scalar[0:n_max]
Y_test_scalar = Y_test_scalar[0:n_max]

mnist_label_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

"""### Réduction de la taille des images

On utilise la fonction ```resize``` de Tensorflow
"""

def downsample(x):
    return tf.image.resize(tf.constant(x[tf.newaxis, ..., tf.newaxis]), [int(0.5*x.shape[0]), int(0.5*x.shape[1])], method='lanczos3').numpy()[0, :, :, 0]


X_train_down = np.array([downsample(img) for img in X_train])
X_test_down = np.array([downsample(img) for img in X_test])

"""### Préparation des données

On permute les entrées par défaut de MNIST avec nos données downsamplées pour s'adapter au modèle et on s'assure que toutes les images ont le même format
"""

X_train, Y_train, X_test, Y_test = X_train_down, X_train, X_test_down, X_test

img_rows, img_cols, nb_channels = X_train.shape[1], X_train.shape[2], 1

X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, nb_channels)
X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, nb_channels)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255

Y_train = Y_train.astype('float32')
Y_test = Y_test.astype('float32')
Y_train /= 255
Y_test /= 255

"""### Définition du modèle

On procède par test avec différentes configuration de modèle, jusqu'à aboutir à celle-ci qui semble satisfaisante. On utilise ```ReduceLROnPlateau``` pour adapter le learning rate au fur et à mesuer de l'apprentissage et ```EarlyStopping``` pour arrêter l'entraînement lorsque celui-ci ne fait plus progresser le réseau (i.e qu'il a atteint des performances satisfaisantes ou qu'il est tombé dans un minimum local)
"""

batch_size = 64
model = Sequential()
model.add(Input(shape=X_train[0].shape))
model.add(UpSampling2D(interpolation="bilinear"))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(8, (5, 5), activation='relu', padding='same'))
model.add(Conv2D(1, (7, 7), activation='sigmoid', padding='same'))
model.summary()

"""__Attention__: Si le réseau s'arrête vers l'epoch numéro 10-15, ou que sa loss stationne autour de 10, c'est qu'il est bloqué dans un minimum local, et qu'il faut donc le recompiler ET le réentrainer. Cela arrive malheureusement ~4 fois sur 5"""

learning_rate = 0.01
model.compile(loss='MeanSquaredError', optimizer=optimizers.Adam(learning_rate), metrics=['accuracy'])

reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.3, patience=5, min_lr=0.00001, verbose=1)
early_stopping = EarlyStopping(monitor='loss', min_delta=0.0001, patience=10, verbose=1, mode='min', baseline=None, restore_best_weights=True)

n_epochs = 80
model.fit(X_train, Y_train, epochs=n_epochs, batch_size=batch_size, use_multiprocessing=True, callbacks=[reduce_lr, early_stopping])

score = model.evaluate(X_test, Y_test, verbose=False)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""### Vérification des résultats

On affiche les résultats du réseau de neurone pour comparer
"""

plt.figure(figsize=(20, 8))
for idx in range(0, 5):
    rand_ind = np.random.randint(0, X_train_down.shape[0])

    plt.subplot(3, 10, idx+1)
    plt.imshow(Y_test[rand_ind, :, :], cmap='gray')
    plt.title(mnist_label_list[int(Y_test_scalar[rand_ind])] + ": truth")

    plt.subplot(3, 10, idx+1+10)
    plt.imshow(model(np.expand_dims(X_test[rand_ind, :, :], axis=0))[
               0, :, :, 0], cmap='gray')
    plt.title(mnist_label_list[int(Y_test_scalar[rand_ind])] + ": network")

    plt.subplot(3, 10, idx+1+20)
    plt.imshow(super_res_interpolate(np.expand_dims(
        X_test[rand_ind, :, :], axis=0), delta)[0, :, :, 0], cmap='gray')
    plt.title(mnist_label_list[int(Y_test_scalar[rand_ind])] + ": bilinear")

"""# Evaluation

To evaluate the work, you should rate the code for 
- 1) Importing MNIST correctly (correctly formatting the data) : 1 point
- 2) Creating a model which makes sense (correct input/output sizes) : 1 point
- 3) Training and achieving good results  : 2 points. 1 point if the learning increases but does not reach around $80\%$, 2 points if the learning reaches around $80\%$
- 4) Display a visual comparison of your network with ```super_res_interpolate``` for several examples

Total over 5 points.

"""